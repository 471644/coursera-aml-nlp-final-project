{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "Icw_IZQ-sRIH",
    "outputId": "47e5ade8-d1ab-413d-f077-82b93e91b8b8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "VVKz0Ebbpoz9",
    "outputId": "51d37ff0-6ce4-4cf8-c486-35bff4380ab1"
   },
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/hse-aml/natural-language-processing/master/setup_google_colab.py -O setup_google_colab.py\n",
    "import setup_google_colab\n",
    "setup_google_colab.setup_honor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "WDkCFcc-32k4",
    "outputId": "f12fbe20-cbda-489b-bedd-bbbddfdf8b5c"
   },
   "outputs": [],
   "source": [
    "! sh download_cornell.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45Ga9n6bBdsI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from math import ceil\n",
    "from random import shuffle\n",
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QTiuwRQ3O_sF"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TFaK819DB11F",
    "outputId": "87d8005c-2074-4eb7-9685-17565a9dd224"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.constraints import max_norm\n",
    "\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "830g96T7h5lP"
   },
   "source": [
    "# Downoad and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3j2sv9C_xZL0"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "u4Vm6osjEulU",
    "outputId": "7d13902e-6a53-4574-c161-02a7eff47ee2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83097/83097 [00:02<00:00, 28369.76it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import *\n",
    "\n",
    "def extractText(line, fast_preprocessing=True):\n",
    "    tag_re = re.compile(r'(<!--.*?-->|<[^>]*>)')\n",
    "    \n",
    "    line = tag_re.sub('', line)\n",
    "    line = line.replace('\\n', '')\n",
    "    line = re.sub('\\s+', ' ', line)\n",
    "    line = line.strip()\n",
    "    \n",
    "    return line\n",
    "\n",
    "def splitConversations(conversations, max_len=20, fast_preprocessing=True):\n",
    "    data = []\n",
    "    for i, conversation in enumerate(tqdm(conversations)):\n",
    "        lines = conversation['lines']\n",
    "        for i in range(len(lines) - 1):\n",
    "            request = extractText(lines[i]['text'])\n",
    "            reply = extractText(lines[i + 1]['text'])\n",
    "            if 0 < len(request) <= max_len and 0 < len(reply) <= max_len:\n",
    "                data += [(request, reply)]\n",
    "    return data\n",
    "\n",
    "\n",
    "def readCornellData(path, max_len=20, fast_preprocessing=True):\n",
    "    dataset = CornellData(path)\n",
    "    conversations = dataset.getConversations()\n",
    "    return splitConversations(conversations, max_len=max_len, fast_preprocessing=fast_preprocessing)\n",
    "\n",
    "data = readCornellData('data/cornell', max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cxhh886BjZUl"
   },
   "outputs": [],
   "source": [
    "ALPHABET = set()\n",
    "for c, r in data:\n",
    "    ALPHABET.update(c)\n",
    "    ALPHABET.update(r)\n",
    "  \n",
    "ALPHABET = sorted(ALPHABET)\n",
    "\n",
    "START_SYMBOL = 'START'\n",
    "END_SYMBOL = 'END'\n",
    "PAD_SYMBOL = 'PAD'\n",
    "SPECIAL_CHARACHTERS = [PAD_SYMBOL, START_SYMBOL, END_SYMBOL]\n",
    "\n",
    "char2id = {c:i for i, c in enumerate(SPECIAL_CHARACHTERS + ALPHABET)}\n",
    "id2char = {i:c for i, c in enumerate(SPECIAL_CHARACHTERS + ALPHABET)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "g5GmIRgsg6_u",
    "outputId": "71e064a2-2370-4e65-9f4d-630e54a9e2d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume of data: 51539\n"
     ]
    }
   ],
   "source": [
    "print('Volume of data:', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WFrMT_JgYsJT"
   },
   "outputs": [],
   "source": [
    "# sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GsCbJQl6h5sA"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HAPqLjCN_rHb"
   },
   "outputs": [],
   "source": [
    "def text2seq(text, char2id, max_len):\n",
    "    start = [char2id[START_SYMBOL]]\n",
    "    chars_ids = [char2id[text[i]] for i in range(min(max_len - 2, len(text)))]\n",
    "    end = [char2id[END_SYMBOL]]\n",
    "    padding = [char2id[PAD_SYMBOL]] * max(0, max_len - len(text) - 2) \n",
    "\n",
    "    return start + chars_ids + end + padding\n",
    "\n",
    "def seq2text(seq, id2char, remove_special=True):\n",
    "    text = ''.join(map(id2char.get, seq))\n",
    "\n",
    "    if remove_special:\n",
    "        for spc in SPECIAL_CHARACHTERS:\n",
    "            text = text.replace(spc, ' ')\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JHDbgS2ofVLR"
   },
   "outputs": [],
   "source": [
    "text = 'Wubba lubba dub-dub!'\n",
    "assert(seq2text(text2seq(text, char2id, MAX_LEN), id2char) == text)\n",
    "assert(seq2text(text2seq(text, char2id, len(text)), id2char) == text[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ckbFs3OxLEel"
   },
   "outputs": [],
   "source": [
    "def baseline_generator(data, batch_size):\n",
    "    n_steps = ceil(len(data) / batch_size)\n",
    "    while True:\n",
    "        shuffle(data)\n",
    "\n",
    "        for i in range(n_steps):\n",
    "            contexts, repsponses = zip(*data[i*batch_size:(i+1)*batch_size])\n",
    "\n",
    "            c_batch = np.array([text2seq(c, char2id, MAX_LEN) for c in contexts])\n",
    "            c_batch_shifted = np.expand_dims(np.hstack([c_batch[:, 1:], np.full((c_batch.shape[0], 1), char2id[PAD_SYMBOL])]), axis=-1) \n",
    "\n",
    "            r_batch = np.array([text2seq(r, char2id, MAX_LEN) for r in repsponses])\n",
    "            r_batch_shifted = np.expand_dims(np.hstack([r_batch[:, 1:], np.full((r_batch.shape[0], 1), char2id[PAD_SYMBOL])]), axis=-1)\n",
    "\n",
    "            yield ([c_batch, r_batch], [c_batch_shifted, r_batch_shifted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d04Wj6RvzM6n"
   },
   "outputs": [],
   "source": [
    "# batch_size_ = 128\n",
    "# (context, response), (context_shifted, response_shifted) = next(baseline_generator(data, batch_size_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-CV9DGGH9Aq"
   },
   "outputs": [],
   "source": [
    "def get_masked_loss(mask_value):\n",
    "    mask_value = K.variable(mask_value)\n",
    "    def masked_categorical_crossentropy(y_true, y_pred):\n",
    "        mask = K.all(K.equal(y_true, mask_value), axis=-1)\n",
    "        mask = 1 - K.cast(mask, K.floatx())\n",
    "        loss = K.sparse_categorical_crossentropy(y_true, y_pred) * mask\n",
    "        return K.sum(loss) / K.sum(mask)\n",
    "    return masked_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H31oITr5Tojv"
   },
   "outputs": [],
   "source": [
    "def GCA_response(encoder, decoder, context, max_steps=MAX_LEN):\n",
    "  \n",
    "    rnn_state = [np.zeros((1, LATENT_DIM))] * len(encoder.outputs)\n",
    "\n",
    "    context = np.array(text2seq(context, char2id, MAX_LEN)).reshape(1, -1)\n",
    "    rnn_state = encoder.predict([context] + rnn_state)\n",
    "    if not isinstance(rnn_state, list):\n",
    "        rnn_state = [rnn_state]\n",
    "    \n",
    "    utterance_partial = np.full((1, MAX_LEN), char2id[PAD_SYMBOL])\n",
    "    utterance_partial[0, 0] = char2id[START_SYMBOL]\n",
    "    \n",
    "    utterance = []\n",
    "    \n",
    "    for i in range(1, min(max_steps, MAX_LEN)):\n",
    "        output_tokens, *rnn_state = decoder_model.predict([utterance_partial] + rnn_state)\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0, 0])\n",
    "        if sampled_token_index == char2id[END_SYMBOL]: break\n",
    "          \n",
    "        utterance.append(sampled_token_index)\n",
    "        utterance_partial[0, 0] = sampled_token_index\n",
    "            \n",
    "    text = seq2text(utterance, id2char, remove_special=False)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FIrYwIXyh5yo"
   },
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DQmeA6O1J9Km"
   },
   "outputs": [],
   "source": [
    "BASELINE_WEIGHTS_PATH = '/content/gdrive/My Drive/Colab Notebooks/coursera/advanced_machine_learning/nlp/baseline_GCA_char_cornell.h5py'\n",
    "\n",
    "LATENT_DIM = 256 + 128\n",
    "BATCH_SIZE = 1024\n",
    "VOCAB_SIZE = len(char2id)\n",
    "EMBEDDINGS_DIM = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zWiIkQraEC4_"
   },
   "source": [
    "Shared layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wtwPwCReEE4g"
   },
   "outputs": [],
   "source": [
    "inp_context = Input(shape=(MAX_LEN,), dtype='int32', name='input_context')\n",
    "inp_reply = Input(shape=(MAX_LEN,), dtype='int32', name='input_utterance')\n",
    "\n",
    "embeddings = Embedding(output_dim=EMBEDDINGS_DIM, \n",
    "                       input_dim=VOCAB_SIZE, \n",
    "                       mask_zero=True, \n",
    "                       name='char_embeddings')\n",
    "\n",
    "encoder_input = embeddings(inp_context)\n",
    "decoder_input = embeddings(inp_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RLFUaCEZEG7m"
   },
   "source": [
    "Bot layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wSzAYq3WAK2A"
   },
   "outputs": [],
   "source": [
    "rnn_layer = GRU(LATENT_DIM, \n",
    "                  return_sequences=True,\n",
    "                  return_state=True,\n",
    "                  name='sequence_modeller')\n",
    "\n",
    "output_dense = Dense(VOCAB_SIZE, activation='softmax', name='output')\n",
    "\n",
    "encoder_outputs, *encoder_states = rnn_layer(encoder_input)\n",
    "encoder_outputs = output_dense(encoder_outputs)\n",
    "encoder_outputs = Lambda(lambda x: x, name='context_modelling')(encoder_outputs)\n",
    "\n",
    "decoder_outputs, *decoder_states = rnn_layer(decoder_input, initial_state=encoder_states)\n",
    "decoder_outputs = output_dense(decoder_outputs)\n",
    "decoder_outputs = Lambda(lambda x: x, name='reply_modelling')(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "IRfQKNnIB_o9",
    "outputId": "53b9ae8e-a2f5-44cd-c5d6-da178350aa53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_context (InputLayer)      (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embeddings (Embedding)     (None, 32, 16)       1776        input_context[0][0]              \n",
      "                                                                 input_utterance[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_utterance (InputLayer)    (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequence_modeller (GRU)         [(None, 32, 384), (N 461952      char_embeddings[0][0]            \n",
      "                                                                 char_embeddings[1][0]            \n",
      "                                                                 sequence_modeller[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 32, 111)      42735       sequence_modeller[0][0]          \n",
      "                                                                 sequence_modeller[1][0]          \n",
      "__________________________________________________________________________________________________\n",
      "context_modelling (Lambda)      (None, 32, 111)      0           output[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reply_modelling (Lambda)        (None, 32, 111)      0           output[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 506,463\n",
      "Trainable params: 506,463\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bot_model = Model([inp_context, \n",
    "                   inp_reply], \n",
    "                  [encoder_outputs, \n",
    "                   decoder_outputs], name='generative_conversational_agent')\n",
    "bot_model.compile(optimizer=Adam(decay=1e-6, clipnorm=1.), \n",
    "                  loss=get_masked_loss(char2id[PAD_SYMBOL]))\n",
    "bot_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cQQhA03K_SkG"
   },
   "outputs": [],
   "source": [
    "if isinstance(rnn_layer, LSTM):\n",
    "    encoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
    "    encoder_state_input_c = Input(shape=(LATENT_DIM,))\n",
    "    encoder_states_inputs = [encoder_state_input_h, encoder_state_input_c]\n",
    "\n",
    "    _, *encoder_states = rnn_layer(encoder_input, initial_state=encoder_states_inputs)\n",
    "    # encoder_states = [state_h, state_c]\n",
    "\n",
    "    encoder_model = Model([inp_context] + encoder_states_inputs, encoder_states, name='bot_encoder')\n",
    "  \n",
    "elif isinstance(rnn_layer, GRU):\n",
    "    encoder_state_input = Input(shape=(LATENT_DIM,))\n",
    "    _, encoder_state = rnn_layer(encoder_input, initial_state=[encoder_state_input])\n",
    "\n",
    "    encoder_model = Model([inp_context, encoder_state_input], encoder_state, name='bot_encoder')\n",
    "  \n",
    "else:\n",
    "    raise NotImplementedError(\"Use LSTM or GRU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IE6-eHFol979"
   },
   "outputs": [],
   "source": [
    "if isinstance(rnn_layer, LSTM):\n",
    "    decoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
    "    decoder_state_input_c = Input(shape=(LATENT_DIM,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "    decoder_outputs, *decoder_states = rnn_layer(decoder_input, initial_state=decoder_states_inputs)\n",
    "    decoder_outputs = output_dense(decoder_outputs)\n",
    "\n",
    "    decoder_model = Model([inp_reply] + decoder_states_inputs, [decoder_outputs] + decoder_states, name='bot_decoder')\n",
    "  \n",
    "elif isinstance(rnn_layer, GRU):\n",
    "    decoder_state_input = Input(shape=(LATENT_DIM,))\n",
    "\n",
    "    decoder_outputs, decoder_state = rnn_layer(decoder_input, initial_state=[decoder_state_input])\n",
    "    decoder_outputs = output_dense(decoder_outputs)\n",
    "\n",
    "    decoder_model = Model([inp_reply, decoder_state_input], [decoder_outputs, decoder_state], name='bot_decoder')\n",
    "  \n",
    "else:\n",
    "    raise NotImplementedError(\"Use LSTM or GRU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8A-pk3dxAqR"
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(BASELINE_WEIGHTS_PATH):\n",
    "    bot_model.load_weights(BASELINE_WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "faE3umKLETVp"
   },
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8UOAXh5DSvFR"
   },
   "outputs": [],
   "source": [
    "class SamplingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, data, n_samples, name=''):\n",
    "        self.data = data\n",
    "        self.n_samples = n_samples\n",
    "        self.name = name\n",
    "      \n",
    "    @staticmethod\n",
    "    def print_sample(encoder, decoder, data, max_steps=MAX_LEN, decode_fn=GCA_response):\n",
    "        context, response = data[np.random.randint(len(data))]\n",
    "        pred_text = decode_fn(encoder_model, \n",
    "                              decoder_model, \n",
    "                              context, \n",
    "                              max_steps=max_steps)\n",
    "\n",
    "        print('CONTEXT:', context)\n",
    "        print('GT:', response)\n",
    "        print('PRED:', pred_text)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        print(self.name)\n",
    "        for i in range(self.n_samples):\n",
    "            self.__class__.print_sample(encoder_model, decoder_model, self.data)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "r4OlWFE1ntLX",
    "outputId": "3d3c62e0-aad8-400e-8b9c-6c1f84c97afd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT: Who's that?\n",
      "GT: It's me - Lothar. Are you okay?\n",
      "PRED: What do you mean?\n"
     ]
    }
   ],
   "source": [
    "SamplingCallback.print_sample(encoder_model, decoder_model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqmiY_aFO1jo"
   },
   "outputs": [],
   "source": [
    "data_generator = baseline_generator(data, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdHqk1Z4OrH1"
   },
   "outputs": [],
   "source": [
    "bot_model.fit_generator(data_generator, steps_per_epoch=10 * len(data) // BATCH_SIZE,\n",
    "                           epochs=128, verbose=1, \n",
    "                           callbacks=[SamplingCallback(data, 3, 'Sample check'), \n",
    "                                      ModelCheckpoint(BASELINE_WEIGHTS_PATH, monitor='loss', save_best_only=True, save_weights_only=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7tN8twgCh6BK"
   },
   "source": [
    "# Inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0f-dqPjLTrpM"
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(BASELINE_WEIGHTS_PATH):\n",
    "    bot_model.load_weights(BASELINE_WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "aw_G3Fxu0bBs",
    "outputId": "a63f1814-efd7-4a58-a14b-4e3753ffe18e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:Hello!\n",
      "Bot: Hello, sweetheart.\n",
      "You:How are you?\n",
      "Bot: Fine. You like it.\n",
      "You:What is your name?\n",
      "Bot: My name is Miles.\n",
      "You:How old are you?\n",
      "Bot: Fine.\n",
      "You:How tall are you?\n",
      "Bot: Fine.\n",
      "You:Do you like me?\n",
      "Bot: Yes, you do recall.\n",
      "You:Did you enjoy your training schedule?\n",
      "Bot: Want to take a dollar wearon?\n",
      "You:I will consider it as positive answer\n",
      "Bot: Accident.\n",
      "You:exit\n",
      "Bot: Please.\n"
     ]
    }
   ],
   "source": [
    "context = ''\n",
    "while context != 'exit':\n",
    "    context = input('You:')\n",
    "    response = GCA_response(encoder_model, decoder_model, context)\n",
    "    print('Bot:', response)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "honor_char_cornell.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
